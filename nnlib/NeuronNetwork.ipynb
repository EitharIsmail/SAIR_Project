{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f52ffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f332d635",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuron:\n",
    "    \n",
    "    def __init__(self, n_inputs, activation, Lr = 0.01):\n",
    "        self.w = np.random.randn(n_inputs, 1)\n",
    "        self.b = np.zeros((1, 1))\n",
    "        self.lose =[] #for viz\n",
    "        self.activation = activation\n",
    "        self.Lr = Lr\n",
    "        self.n_inputs = n_inputs\n",
    "\n",
    "\n",
    "    #activation functions\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -250, 250)))\n",
    "    \n",
    "\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "\n",
    "    def tanh(self, z):\n",
    "        return np.tanh(z) \n",
    "    \n",
    "\n",
    "    #forward path to activate the neuron\n",
    "    def forward(self, X):\n",
    "\n",
    "        self.X = X\n",
    "        z = X @ self.w + self.b\n",
    "\n",
    "        #calling activation functions to activate the neuron to produce output(prediction)\n",
    "        if self.activation == \"Linear\":\n",
    "            self.y_pred = z\n",
    "        elif self.activation == \"Relu\":\n",
    "            self.y_pred = self.relu(z)\n",
    "        elif self.activation == \"Tanh\":\n",
    "            self.y_pred = self.tanh(z)\n",
    "        elif self.activation == \"Sigmoid\":\n",
    "            self.y_pred = self.sigmoid(z)\n",
    "\n",
    "\n",
    "        return self.y_pred\n",
    "    \n",
    "\n",
    "\n",
    "    def loss(self, los, y_pred, Y_true):\n",
    "        if self.los == \"MSE\":\n",
    "            L = (Y_true - y_pred) ** 2\n",
    "        elif self.los == \"BCE\":\n",
    "            L = -[Y_true * np.log(y_pred) + (1-Y_true) * np.log(1-y_pred)]\n",
    "        \n",
    "        return L\n",
    "\n",
    "\n",
    "\n",
    "    # backword --> calculate gradients\n",
    "    def backword(self, los, y_pred, Y_true):\n",
    "\n",
    "        #loss function first\n",
    "        L = self.loss(los, y_pred, Y_true)\n",
    "        #filling the loss list for viz\n",
    "        self.lose.append(L)\n",
    "        \n",
    "        # global deriviatives\n",
    "        dL_dy = 0\n",
    "        if self.loss == \"MSE\":\n",
    "            dy_dz = 1\n",
    "            dL_dy = 2 * (Y_true - y_pred) * dy_dz\n",
    "        elif self.loss == \"BCE\":\n",
    "            dL_dy = (-(Y_true / y_pred + (1- Y_true)/(1-y_pred))) * dy_dz  # this is temporar till I find the correct answer\n",
    "\n",
    "        \n",
    "        dL_dw = dL_dy @ self.dw\n",
    "        dL_db = np.sum(dL_dy) * self.db\n",
    "        #dL_dx = self.dL_dy * self.dx\n",
    "\n",
    "        return dL_dw, dL_db\n",
    "\n",
    "    def step(self, dL_dw, dL_db):\n",
    "        #update parameters\n",
    "        self.w = self.w - self.Lr * dL_dw\n",
    "        self.b = self.b - self.Lr * dL_db\n",
    "\n",
    "        return self.w, self.b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605860b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DesnsLayer:\n",
    "#adding the w initialization\n",
    "    #n_output mean the number of neurons per layer\n",
    "    def __init__(self, n_inputs, n_outputs, activation, los, Lr = 0.01, regularization='none', lambda_val=0.01, dropout_rate=0.0):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.activation = activation\n",
    "        #self.w = np.random.randn(n_inputs, n_outputs) * 0.1 # here we multiplied by 0.1 to avoid the explding of the gradients because if the values of w came large then z will be big\n",
    "        self.w = self.initialize_parameters(self.n_inputs, self.n_outputs, self.activation)\n",
    "        self.b = np.zeros((1, n_outputs))\n",
    "        self.Lr = Lr\n",
    "        self.los = los \n",
    "        self.lose =[] #for viz\n",
    "        self.lambda_val = lambda_val \n",
    "        self.regularization = regularization\n",
    "\n",
    "\n",
    "    def initialize_parameters(self, n_inputs, n_outputs, activation):\n",
    "            self.activation = activation\n",
    "            self.n_inputs = n_inputs\n",
    "            self.n_outputs = n_outputs\n",
    "            if self.activation == 'Sigmoid' or self.activation == 'Tanh' :\n",
    "                scale = np.sqrt(2.0 / (self.n_inputs + self.n_outputs))\n",
    "                np.random.randn(self.n_inputs, n_outputs) * scale\n",
    "                return np.random.randn(self.n_inputs, self.n_outputs) * 0.1\n",
    "            elif self.activation == 'Relu':\n",
    "                scale = np.sqrt(2.0 / self.n_inputs)\n",
    "                np.random.randn(self.n_inputs) * scale\n",
    "                return np.random.randn(self.n_inputs, self.n_outputs) * 0.1\n",
    "    \n",
    "    #activation functions\n",
    "    def sigmoid(self, X):\n",
    "        return 1 / (1 + np.exp(-np.clip(X, -250, 250)))\n",
    "    \n",
    "    def relu(self, X):\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def tanh(self, X):\n",
    "        return np.tanh(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.z = X @ self.w + self.b\n",
    "        #print(z)\n",
    "\n",
    "        #calling activation functions to activate the neuron to produce output(prediction)\n",
    "        if self.activation == \"Linear\":\n",
    "            self.y_pred = self.z\n",
    "        elif self.activation == \"Relu\":\n",
    "            self.y_pred = self.relu(self.z)\n",
    "        elif self.activation == \"Tanh\":\n",
    "            self.y_pred = self.tanh(self.z)\n",
    "        elif self.activation == \"Sigmoid\":\n",
    "            self.y_pred = self.sigmoid(self.z)\n",
    "\n",
    "        return self.y_pred\n",
    "\n",
    "\n",
    "    def loss(self, los, y_pred, Y_true):\n",
    "        dL_dy = 0\n",
    "        if los == \"MSE\":\n",
    "            L = np.mean((Y_true - y_pred) ** 2)\n",
    "            self.lose.append(L)\n",
    "            n = Y_true.shape[0]\n",
    "            dL_dy = (2 /n) * (Y_true - y_pred)\n",
    "        elif los == \"BCE\":\n",
    "            # use the clip\n",
    "            epsilon = 1e-15\n",
    "            y_pred = np.clip(y_pred, epsilon, 1 - epsilon) #to avoid log(0)\n",
    "            L = - np.mean(Y_true * np.log(y_pred) + (1 - Y_true) * np.log(1 - y_pred))\n",
    "            self.lose.append(L)\n",
    "            dL_dy = (y_pred - Y_true) / Y_true.shape[0]\n",
    "            #(y_pred * (1- y_pred) * len(Y_true))\n",
    "        \n",
    "        return dL_dy\n",
    "\n",
    "\n",
    "    # backword --> calculate gradients\n",
    "    def backword(self, dL_dy):\n",
    "\n",
    "        # activations derivatieves\n",
    "        dy_dz = 0\n",
    "        if self.activation == \"Linear\":\n",
    "            dy_dz = 1\n",
    "        elif self.activation == \"Relu\":\n",
    "            dy_dz = (self.z > 0).astype(float)\n",
    "        elif self.activation == \"Tanh\":\n",
    "            dy_dz = 1 - self.y_pred**2\n",
    "        elif self.activation == \"Sigmoid\":\n",
    "            dy_dz = self.y_pred * (1 - self.y_pred)\n",
    "        \n",
    "        dL_dz = dL_dy * dy_dz\n",
    "        dL_dw = self.X.T @ dL_dz\n",
    "        dL_db = np.sum(dL_dz, axis=0, keepdims=1) * 1\n",
    "        dL_dx = dL_dz @ self.w.T\n",
    "\n",
    "         # Add regularization gradient to enhance dl_dw to avoid overfitting / the gradient of the summation and abs is the sign\n",
    "        if self.regularization == 'L2':\n",
    "            dL_dw = dL_dw + ((self.lambda_val / self.X.shape[0]) * self.w)\n",
    "        elif self.regularization == 'L1':\n",
    "            dL_dw = dL_dw + ((self.lambda_val / self.X.shape[0]) * np.sign(self.w))\n",
    "            \n",
    "\n",
    "        return dL_dw, dL_db, dL_dx\n",
    "\n",
    "    def step(self, dL_dw, dL_db):\n",
    "        #update parameters\n",
    "        self.w = self.w - self.Lr * dL_dw\n",
    "        self.b = self.b - self.Lr * dL_db\n",
    "\n",
    "        return self.w, self.b\n",
    " \n",
    "\n",
    "    def viz(self):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.title(\"ðŸ§  Neural Layer Architecture\", fontsize=14, pad=20)\n",
    "\n",
    "        # Draw neurons\n",
    "        for i in range(self.n_inputs):  # Input layer\n",
    "            plt.scatter(0, i, s=500, c='lightblue', edgecolors='black', zorder=5)\n",
    "            plt.text(0, i, f'x{i+1}', ha='center', va='center', fontweight='bold')\n",
    "            \n",
    "        for j in range(self.n_outputs):  # Output layer\n",
    "            plt.scatter(2, j, s=500, c='lightgreen', edgecolors='black', zorder=5)\n",
    "            plt.text(2, j, f'a{j+1}', ha='center', va='center', fontweight='bold')\n",
    "            \n",
    "            # Draw connections\n",
    "            for i in range(self.n_inputs):\n",
    "                plt.plot([0, 2], [i, j], 'gray', alpha=0.3)\n",
    "\n",
    "        plt.xlim(-0.5, 2.5)\n",
    "        plt.ylim(-0.5, 4.5)\n",
    "        plt.axis('off')\n",
    "        plt.text(0, -0.3, f\"Input Layer {self.n_inputs}, neurons\", ha='center', fontsize=12)\n",
    "        plt.text(2, -0.3, f\"Dense Layer {self.n_outputs}, neurons\", ha='center', fontsize=12)\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d29681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7c779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model building stack of layers MLP\n",
    "class MLP:\n",
    "#dropout_rate=0.0\n",
    "    def __init__(self, n_neuorns_per_layer, list_activations=None, Lr = 0.01,  regularization='none', lambda_val=0.01):\n",
    "    \n",
    "        #item in zero index is the intput it self\n",
    "        self.n_neuorns_per_layer = n_neuorns_per_layer\n",
    "        self.list_activations = list_activations or [\"Relu\"] * (len(n_neuorns_per_layer) - 2) + [\"Sigmoid\"]\n",
    "        #to stack layers in it\n",
    "        self.layers = [] \n",
    "        self.lose = []\n",
    "        self.lambda_val = lambda_val \n",
    "        self.regularization = regularization\n",
    "        self.Lr = Lr\n",
    "        \n",
    "        #for the first layer is X it self and it is not counted in the loop as a layer, I see it as a layer but the program doesn't, it see the second layer\n",
    "        # we said - 1 to skip first layer as it is the input\n",
    "        for i in range(len(n_neuorns_per_layer) - 1): \n",
    "            self.layer = DesnsLayer(self.n_neuorns_per_layer[i], self.n_neuorns_per_layer[i + 1], self.list_activations[i], self.Lr, self.regularization, self.lambda_val)\n",
    "            #creating a list of objects to call it later to access the class functions\n",
    "            self.layers.append(self.layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        #to save the input to the memeory to know it later\n",
    "        self.X = X \n",
    "\n",
    "        #because for each layer its input is the output of previous layer\n",
    "        self.previous_output = X \n",
    "        for layer in self.layers:\n",
    "            self.previous_output = self.layer.forward(self.previous_output)\n",
    "        #return the last output as the inner ones are not important to me\n",
    "        return self.previous_output  \n",
    "    \n",
    "    def backward(self, dL_dy):\n",
    "        y_pred = self.previous_output\n",
    "\n",
    "        parameters_gradient =[]\n",
    "        current_grad = dL_dy\n",
    "        for layer in reversed(self.layers):\n",
    "            dL_dw, dL_db, dL_dx = self.layer.backword(current_grad)\n",
    "            parameters_gradient.append((dL_dw, dL_db))\n",
    "            current_grad = dL_dx\n",
    "\n",
    "        return list(reversed(parameters_gradient))\n",
    "    \n",
    "    def step(self, parameters_gradient, Lr=0.01):\n",
    "        for i, (dL_dw, dL_db) in enumerate(parameters_gradient):\n",
    "            self.layers[i].w -= Lr * dL_dw\n",
    "            self.layers[i].b -= Lr * dL_db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1e0ffc",
   "metadata": {},
   "source": [
    "Dropout:\n",
    "\n",
    "Deep Learning Connection: By randomly \"banning\" (dropping) neurons during training, we force the network to be robust. It cannot rely on just one specific input or pathway. It spreads the knowledge across all neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a202fac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the classiifer\n",
    "class loss_functions:\n",
    "    def __init__(self, Y_true, y_pred):\n",
    "        self.Y_true = Y_true\n",
    "        self.y_pred = y_pred\n",
    "\n",
    "    def MSE(self, Y_true, y_pred):\n",
    "        L = np.mean((Y_true - y_pred) ** 2)\n",
    "        return L\n",
    "\n",
    "    def MSE_grad(self, Y_true, y_pred):\n",
    "        n = Y_true.shape[0]\n",
    "        L_grad = (2 /n) * (Y_true - y_pred)\n",
    "        return L_grad\n",
    "\n",
    "    def binary_cross_entropy(self, y_pred, y_true):\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon) #to avoid log(0)\n",
    "        L = - np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return L\n",
    "    \n",
    "    #its gradiant\n",
    "    def binary_cross_entropy_grad(self, y_pred, y_true):\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        L = (y_pred - y_true) / (y_pred * (1 - y_pred) * len(y_true))\n",
    "        return L\n",
    "    \n",
    "\n",
    "class NeruronNetworkLearning:\n",
    "\n",
    "    def __init__(self, layer_size, los,  activations = None, Lr = 0.01, batch_size = 32, regularization='none', lambda_val=0.01, dropout_rate=0.0):\n",
    "        self.mlp = MLP(layer_size, activations, regularization, lambda_val)\n",
    "        self.layer_size = layer_size\n",
    "        self.activations = activations\n",
    "        self.batch_size = batch_size\n",
    "        self.Lr = Lr\n",
    "        self.loss_history = []\n",
    "        self.accuracy_history = []\n",
    "        self.eval_loss_history = []\n",
    "        self.eval_acc_history = []\n",
    "        self.lambda_val = lambda_val \n",
    "        self.regularization = regularization\n",
    "        self.los = los\n",
    "\n",
    "######### Regularaiation ########    \n",
    "    def reg_loss(self, loss, reg_x):\n",
    "        self.reg_x = reg_x\n",
    "        #calcualte the weights of each layer\n",
    "        reg_w = 0.0\n",
    "        for layer in self.mlp.layers:\n",
    "            if self.regularization == 'L2':\n",
    "                reg_w += np.sum(layer.w**2)\n",
    "            elif self.regularization == 'L1':\n",
    "                reg_w += np.sum(np.abs(layer.w))\n",
    "   \n",
    "        if self.regularization == 'L2':\n",
    "            #explain the formula in the markdown\n",
    "            reg_loss = loss + ((self.lambda_val / (2 * self.reg_x[0])) * (reg_w))\n",
    "        elif self.regularization == 'L1':\n",
    "            reg_loss = loss + ((self.lambda_val / self.reg_x[0]) * (reg_w))\n",
    "        else:\n",
    "            reg_loss = loss\n",
    "        \n",
    "        return reg_loss\n",
    "\n",
    "########## Training the data by looping through epochs and dividing it to batches for initial training\n",
    "    def train(self, X_train, y_train, x_val, y_val, epochs = 1000, verbose = True):\n",
    "        self.X_train = X_train\n",
    "        n_samples = X_train.shape[0]\n",
    "        self.y_train = y_train\n",
    "        self.los_object = loss_functions(self.y_train, self.mlp.layer.y_pred)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "             #to avoid learning one type of output, it will memoeries leading to overtting, also we didn't use suffle function because it will break the dataset sequance\n",
    "            index = np.random.permutation(n_samples)\n",
    "            x_shaffeled = X_train[index]\n",
    "            y_shaffeled = y_train[index]\n",
    "\n",
    "            #loop to create batchs and train them\n",
    "            for start_idx in range(0, n_samples, self.batch_size):\n",
    "                end_index = min(start_idx + self.batch_size, n_samples)\n",
    "                #slizing the dataset based on the batch size\n",
    "                x_batch = x_shaffeled[start_idx : end_index]\n",
    "                y_batch = y_shaffeled[start_idx: end_index]\n",
    "\n",
    "                y_predd = self.mlp.forward(x_batch)\n",
    "\n",
    "                dL_dy = 0.0\n",
    "                if self.los == \"MSE\":\n",
    "                    dL_dy = self.los_object.MSE_grad(y_predd, y_batch)\n",
    "                elif self.los == \"BCE\":\n",
    "                    dL_dy = self.los_object.binary_cross_entropy_grad(y_predd, y_batch)\n",
    "                #dL_dy = self.los_object.binary_cross_entropy_grad(y_predd, y_batch)\n",
    "\n",
    "                gradiants = self.mlp.backward(dL_dy)\n",
    "                \n",
    "                self.mlp.step(gradiants, self.Lr)\n",
    "            \n",
    "            ### Rerunning the whole data on the network to check if it learned\n",
    "            y_pred_full_train = self.mlp.forward(X_train)\n",
    "            full_train_loss1 = 0.0\n",
    "            if self.los == \"MSE\":\n",
    "                full_train_loss1 = self.los_object.MSE(y_predd, y_train)\n",
    "            elif self.los == \"BCE\":\n",
    "                full_train_loss1 = self.los_object.binary_cross_entropy(y_predd, y_train)\n",
    "            ### Adding the regularaization to the loss\n",
    "            #full_train_loss = self.binary_cross_entropy(y_pred_full_train, y_train)\n",
    "            full_train_loss = self.reg_loss(full_train_loss1, X_train)\n",
    "            full_train_acc = self.accuracy(y_pred_full_train, y_train)\n",
    "\n",
    "            self.loss_history.append(full_train_loss)\n",
    "            self.accuracy_history.append(full_train_acc)\n",
    "\n",
    "            #validating the model larning on new\n",
    "            if x_val is not None and y_val is not None:\n",
    "                y_pred_val = self.mlp.forward(x_val)\n",
    "                val_loss = 0.0\n",
    "                if self.los == \"MSE\":\n",
    "                    val_loss = self.los_object.MSE(y_predd, y_train, x_val)\n",
    "                elif self.los == \"BCE\":\n",
    "                    val_loss = self.los_object.binary_cross_entropy(y_predd, y_train)\n",
    "                #val_loss = self.binary_cross_entropy(y_pred_val, y_val)\n",
    "                val_loss = self.reg_loss(val_loss, x_val)\n",
    "                val_acc = self.accuracy(y_pred_val, y_val)\n",
    "\n",
    "                self.eval_loss_history.append(val_loss)\n",
    "                self.eval_acc_history.append(val_acc)\n",
    "\n",
    "            if verbose and (epoch % 100 == 0 or epoch == epochs - 1):\n",
    "                val_info = f\" | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\" if x_val is not None else \"\"\n",
    "                print(f\"Epoch {epoch:4d} | Loss: {full_train_loss:.4f} | Acc: {full_train_acc:.2f}%{val_info}\")\n",
    "\n",
    "\n",
    "    \n",
    "    def accuracy(self, y_pred, y_true):\n",
    "        y_pred = (y_pred >= 0.5).astype(int)\n",
    "        return np.mean(y_pred == y_true) * 100\n",
    "    \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        y_pred = self.mlp.forward(X)\n",
    "        return (y_pred >= 0.5).astype(int)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict probabilities\"\"\"\n",
    "        return self.mlp.forward(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e5743c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š GENERATING DATASETS...\n",
      "Moons dataset: X (500, 2), y (500, 1)\n",
      "Circles dataset: X (500, 2), y (500, 1)\n",
      "\n",
      "Train/Validation splits:\n",
      "Moons - Train: (400, 2), Val: (100, 2)\n",
      "Circles - Train: (400, 2), Val: (100, 2)\n",
      "\n",
      "============================================================\n",
      "ðŸŒ™ TRAINING ON MOONS DATASET\n",
      "============================================================\n",
      "ðŸ§  Neural Network Architecture for Moons:\n",
      "Layer 0: 2 â†’ 16 neurons (Relu)\n",
      "Layer 1: 16 â†’ 8 neurons (Relu)\n",
      "Layer 2: 8 â†’ 1 neurons (Sigmoid)\n",
      "Epoch    0 | Loss: 0.6912 | Acc: 51.75% | Val Loss: 0.6927 | Val Acc: 43.00%\n",
      "Epoch  100 | Loss: 0.0587 | Acc: 98.00% | Val Loss: 0.0664 | Val Acc: 98.00%\n",
      "Epoch  200 | Loss: 0.0071 | Acc: 100.00% | Val Loss: 0.0111 | Val Acc: 100.00%\n",
      "Epoch  300 | Loss: 0.0031 | Acc: 100.00% | Val Loss: 0.0052 | Val Acc: 100.00%\n",
      "Epoch  400 | Loss: 0.0021 | Acc: 100.00% | Val Loss: 0.0049 | Val Acc: 100.00%\n",
      "Epoch  500 | Loss: 0.0014 | Acc: 100.00% | Val Loss: 0.0034 | Val Acc: 100.00%\n",
      "Epoch  600 | Loss: 0.0011 | Acc: 100.00% | Val Loss: 0.0030 | Val Acc: 100.00%\n",
      "Epoch  700 | Loss: 0.0008 | Acc: 100.00% | Val Loss: 0.0024 | Val Acc: 100.00%\n",
      "Epoch  800 | Loss: 0.0007 | Acc: 100.00% | Val Loss: 0.0021 | Val Acc: 100.00%\n",
      "Epoch  900 | Loss: 0.0006 | Acc: 100.00% | Val Loss: 0.0020 | Val Acc: 100.00%\n",
      "Epoch  999 | Loss: 0.0005 | Acc: 100.00% | Val Loss: 0.0019 | Val Acc: 100.00%\n",
      "\n",
      "ðŸŽ¯ Final Moons Validation Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate datasets\n",
    "print(\"ðŸ“Š GENERATING DATASETS...\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# Moons dataset\n",
    "X_moons, y_moons = make_moons(n_samples=500, noise=0.1, random_state=42)\n",
    "y_moons = y_moons.reshape(-1, 1)\n",
    "\n",
    "# Circles dataset  \n",
    "X_circles, y_circles = make_circles(n_samples=500, noise=0.1, factor=0.5, random_state=42)\n",
    "y_circles = y_circles.reshape(-1, 1)\n",
    "\n",
    "print(f\"Moons dataset: X {X_moons.shape}, y {y_moons.shape}\")\n",
    "print(f\"Circles dataset: X {X_circles.shape}, y {y_circles.shape}\")\n",
    "\n",
    "# Split into train/validation sets\n",
    "X_moons_train, X_moons_val, y_moons_train, y_moons_val = train_test_split(\n",
    "    X_moons, y_moons, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_circles_train, X_circles_val, y_circles_train, y_circles_val = train_test_split(\n",
    "    X_circles, y_circles, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain/Validation splits:\")\n",
    "print(f\"Moons - Train: {X_moons_train.shape}, Val: {X_moons_val.shape}\")\n",
    "print(f\"Circles - Train: {X_circles_train.shape}, Val: {X_circles_val.shape}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŒ™ TRAINING ON MOONS DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create and train neural network for moons\n",
    "nn_moons = NeruronNetworkLearning(\n",
    "    layer_size=[2, 16, 8, 1],  # Input: 2, Hidden: 16â†’8, Output: 1\n",
    "    activations=['Relu', 'Relu', 'Sigmoid'],\n",
    "    Lr=0.1,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(\"ðŸ§  Neural Network Architecture for Moons:\")\n",
    "for i, layer in enumerate(nn_moons.mlp.layers):\n",
    "    print(f\"Layer {i}: {layer.w.shape[0]} â†’ {layer.w.shape[1]} neurons ({layer.activation})\")\n",
    "\n",
    "# Train the model\n",
    "nn_moons.train(\n",
    "    X_moons_train, y_moons_train,\n",
    "    x_val=X_moons_val, y_val=y_moons_val,\n",
    "    epochs=1000,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Final evaluation\n",
    "final_pred_moons = nn_moons.predict(X_moons_val)\n",
    "final_proba_moons = nn_moons.predict_proba(X_moons_val)\n",
    "final_acc_moons = nn_moons.accuracy(final_proba_moons, y_moons_val)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Final Moons Validation Accuracy: {final_acc_moons:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c917139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Breast Cancer Dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "def data_loading(Dataset):\n",
    "    #exploring the data\n",
    "    bcd = load_breast_cancer()\n",
    "    #print(bcd.keys())\n",
    "\n",
    "    X = pd.DataFrame(data = bcd.data)\n",
    "    #X = bcd.data\n",
    "    print(X.shape[1])\n",
    "    Y = pd.DataFrame(data = bcd.target)\n",
    "    #print(Y)\n",
    "    #print(X.info)\n",
    "    return X, Y\n",
    "\n",
    "def train_test_split(Dataset):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30b1108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the initionlizer class\n",
    "class WInitializationfunctions:\n",
    "\n",
    "    #shape is a matric that has the diemntions input and output value\n",
    "    \n",
    "    def zeros(shape):\n",
    "        return np.zeros(shape)\n",
    "    \n",
    "    def random(shape, scale = 0.01):\n",
    "        return np.random.randn(*shape) * scale\n",
    "    \n",
    "    def xavier(shape):\n",
    "        n_in, o_in = shape[0], shape[1]\n",
    "        scale = np.sqrt(2.0 / (n_in + o_in))\n",
    "        return np.random.randn(*shape) * scale\n",
    "    \n",
    "    def he(shape):\n",
    "        n_in = shape[0]\n",
    "        scale = np.sqrt(2.0 / n_in)\n",
    "        return np.random.randn(*shape) * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b95b901",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDesnsLayer:\n",
    "\n",
    "    #n_output mean the number of neurons per layer\n",
    "    def __init__(self, n_inputs, n_outputs, activation, Lr = 0.01):\n",
    "        self.w = np.random.randn(n_inputs, n_outputs) * 0.1 # here we multiplied by 0.1 to avoid the explding of the gradients because if the values of w came large then z will be big\n",
    "        self.b = np.zeros((1, n_outputs))\n",
    "        self.activation = activation\n",
    "        self.Lr = Lr\n",
    "        self.lose =[] #for viz\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "\n",
    "    #activation functions\n",
    "    def sigmoid(self, X):\n",
    "        return 1 / (1 + np.exp(-np.clip(X, -250, 250)))\n",
    "    \n",
    "    def relu(self, X):\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def tanh(self, X):\n",
    "        return np.tanh(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.z = X @ self.w + self.b\n",
    "        #print(z)\n",
    "\n",
    "        #calling activation functions to activate the neuron to produce output(prediction)\n",
    "        if self.activation == \"Linear\":\n",
    "            self.y_pred = self.z\n",
    "        elif self.activation == \"Relu\":\n",
    "            self.y_pred = self.relu(self.z)\n",
    "        elif self.activation == \"Tanh\":\n",
    "            self.y_pred = self.tanh(self.z)\n",
    "        elif self.activation == \"Sigmoid\":\n",
    "            self.y_pred = self.sigmoid(self.z)\n",
    "\n",
    "        return self.y_pred\n",
    "    \n",
    "\n",
    "    def loss(self, los, y_pred, Y_true):\n",
    "        dL_dy = 0\n",
    "        if los == \"MSE\":\n",
    "            L = np.mean((Y_true - y_pred) ** 2)\n",
    "            self.lose.append(L)\n",
    "            n = Y_true.shape[0]\n",
    "            dL_dy = (2 /n) * (Y_true - y_pred)\n",
    "        elif los == \"BCE\":\n",
    "            # use the clip\n",
    "            epsilon = 1e-15\n",
    "            y_pred = np.clip(y_pred, epsilon, 1 - epsilon) #to avoid log(0)\n",
    "            L = - np.mean(Y_true * np.log(y_pred) + (1 - Y_true) * np.log(1 - y_pred))\n",
    "            self.lose.append(L)\n",
    "            dL_dy = (y_pred - Y_true) / Y_true.shape[0]\n",
    "            #(y_pred * (1- y_pred) * len(Y_true))\n",
    "        \n",
    "        return dL_dy\n",
    "\n",
    "\n",
    "    # backword --> calculate gradients\n",
    "    def backword(self, dL_dy):\n",
    "\n",
    "        # activations derivatieves\n",
    "        dy_dz = 0\n",
    "        if self.activation == \"Linear\":\n",
    "            dy_dz = 1\n",
    "        elif self.activation == \"Relu\":\n",
    "            dy_dz = (self.z > 0).astype(float)\n",
    "        elif self.activation == \"Tanh\":\n",
    "            dy_dz = 1 - self.y_pred**2\n",
    "        elif self.activation == \"Sigmoid\":\n",
    "            dy_dz = self.y_pred * (1 - self.y_pred)\n",
    "        \n",
    "        dL_dz = dL_dy * dy_dz\n",
    "        dL_dw = self.X.T @ dL_dz\n",
    "        dL_db = np.sum(dL_dz, axis=0, keepdims=1) * 1\n",
    "        dL_dx = dL_dz @ self.w.T\n",
    "\n",
    "        return dL_dw, dL_db, dL_dx\n",
    "\n",
    "    def step(self, dL_dw, dL_db):\n",
    "        #update parameters\n",
    "        self.w = self.w - self.Lr * dL_dw\n",
    "        self.b = self.b - self.Lr * dL_db\n",
    "\n",
    "        return self.w, self.b\n",
    " \n",
    "\n",
    "    def viz(self):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.title(\"ðŸ§  Neural Layer Architecture\", fontsize=14, pad=20)\n",
    "\n",
    "        # Draw neurons\n",
    "        for i in range(self.n_inputs):  # Input layer\n",
    "            plt.scatter(0, i, s=500, c='lightblue', edgecolors='black', zorder=5)\n",
    "            plt.text(0, i, f'x{i+1}', ha='center', va='center', fontweight='bold')\n",
    "            \n",
    "        for j in range(self.n_outputs):  # Output layer\n",
    "            plt.scatter(2, j, s=500, c='lightgreen', edgecolors='black', zorder=5)\n",
    "            plt.text(2, j, f'a{j+1}', ha='center', va='center', fontweight='bold')\n",
    "            \n",
    "            # Draw connections\n",
    "            for i in range(self.n_inputs):\n",
    "                plt.plot([0, 2], [i, j], 'gray', alpha=0.3)\n",
    "\n",
    "        plt.xlim(-0.5, 2.5)\n",
    "        plt.ylim(-0.5, 4.5)\n",
    "        plt.axis('off')\n",
    "        plt.text(0, -0.3, f\"Input Layer {self.n_inputs}, neurons\", ha='center', fontsize=12)\n",
    "        plt.text(2, -0.3, f\"Dense Layer {self.n_outputs}, neurons\", ha='center', fontsize=12)\n",
    "        plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAIR_Courses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

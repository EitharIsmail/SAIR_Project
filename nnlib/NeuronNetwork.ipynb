{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f52ffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f332d635",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuron:\n",
    "    \n",
    "    def __init__(self, n_inputs, activation, Lr = 0.01):\n",
    "        self.w = np.random.randn(n_inputs, 1)\n",
    "        self.b = np.zeros((1, 1))\n",
    "        self.lose =[] #for viz\n",
    "        self.activation = activation\n",
    "        self.Lr = Lr\n",
    "        self.n_inputs = n_inputs\n",
    "\n",
    "\n",
    "    #activation functions\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -250, 250)))\n",
    "    \n",
    "\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "\n",
    "    def tanh(self, z):\n",
    "        return np.tanh(z) \n",
    "    \n",
    "\n",
    "\n",
    "    #forward path to activate the neuron\n",
    "    def forward(self, X):\n",
    "\n",
    "        self.X = X\n",
    "        z = X @ self.w + self.b\n",
    "\n",
    "        #calling activation functions to activate the neuron to produce output(prediction)\n",
    "        if self.activation == \"Linear\":\n",
    "            self.y_pred = z\n",
    "        elif self.activation == \"Relu\":\n",
    "            self.y_pred = self.relu(z)\n",
    "        elif self.activation == \"Tanh\":\n",
    "            self.y_pred = self.tanh(z)\n",
    "        elif self.activation == \"Sigmoid\":\n",
    "            self.y_pred = self.sigmoid(z)\n",
    "\n",
    "\n",
    "        return self.y_pred\n",
    "    \n",
    "\n",
    "\n",
    "    def loss(self, los, y_pred, Y_true):\n",
    "        if self.los == \"MSE\":\n",
    "            L = (Y_true - y_pred) ** 2\n",
    "        elif self.los == \"BCE\":\n",
    "            L = -[Y_true * np.log(y_pred) + (1-Y_true) * np.log(1-y_pred)]\n",
    "        \n",
    "        return L\n",
    "\n",
    "\n",
    "\n",
    "    # backword --> calculate gradients\n",
    "    def backword(self, los, y_pred, Y_true):\n",
    "\n",
    "        #loss function first\n",
    "        L = self.loss(los, y_pred, Y_true)\n",
    "        #filling the loss list for viz\n",
    "        self.lose.append(L)\n",
    "        \n",
    "        # global deriviatives\n",
    "        dL_dy = 0\n",
    "        if self.loss == \"MSE\":\n",
    "            dy_dz = 1\n",
    "            dL_dy = 2 * (Y_true - y_pred) * dy_dz\n",
    "        elif self.loss == \"BCE\":\n",
    "            dL_dy = (-(Y_true / y_pred + (1- Y_true)/(1-y_pred))) * dy_dz  # this is temporar till I find the correct answer\n",
    "\n",
    "        \n",
    "        dL_dw = dL_dy @ self.dw\n",
    "        dL_db = np.sum(dL_dy) * self.db\n",
    "        #dL_dx = self.dL_dy * self.dx\n",
    "\n",
    "        return dL_dw, dL_db\n",
    "\n",
    "    def step(self, dL_dw, dL_db):\n",
    "        #update parameters\n",
    "        self.w = self.w - self.Lr * dL_dw\n",
    "        self.b = self.b - self.Lr * dL_db\n",
    "\n",
    "        return self.w, self.b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d29681",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DesnsLayer:\n",
    "\n",
    "    #n_output mean the number of neurons per layer\n",
    "    def __init__(self, n_inputs, n_outputs, activation, Lr = 0.01):\n",
    "        self.w = np.random.randn(n_inputs, n_outputs) * 0.1 # here we multiplied by 0.1 to avoid the explding of the gradients because if the values of w came large then z will be big\n",
    "        self.b = np.zeros((1, n_outputs))\n",
    "        self.activation = activation\n",
    "        self.Lr = Lr\n",
    "        self.lose =[] #for viz\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "\n",
    "    #activation functions\n",
    "    def sigmoid(self, X):\n",
    "        return 1 / (1 + np.exp(-np.clip(X, -250, 250)))\n",
    "    \n",
    "    def relu(self, X):\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def tanh(self, X):\n",
    "        return np.tanh(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.z = X @ self.w + self.b\n",
    "        #print(z)\n",
    "\n",
    "        #calling activation functions to activate the neuron to produce output(prediction)\n",
    "        if self.activation == \"Linear\":\n",
    "            self.y_pred = self.z\n",
    "        elif self.activation == \"Relu\":\n",
    "            self.y_pred = self.relu(self.z)\n",
    "        elif self.activation == \"Tanh\":\n",
    "            self.y_pred = self.tanh(self.z)\n",
    "        elif self.activation == \"Sigmoid\":\n",
    "            self.y_pred = self.sigmoid(self.z)\n",
    "\n",
    "        return self.y_pred\n",
    "\n",
    "\n",
    "    def loss(self, los, y_pred, Y_true):\n",
    "        dL_dy = 0\n",
    "        if los == \"MSE\":\n",
    "            L = np.mean((Y_true - y_pred) ** 2)\n",
    "            self.lose.append(L)\n",
    "            n = Y_true.shape[0]\n",
    "            dL_dy = (2 /n) * (Y_true - y_pred)\n",
    "        elif los == \"BCE\":\n",
    "            # use the clip\n",
    "            epsilon = 1e-15\n",
    "            y_pred = np.clip(y_pred, epsilon, 1 - epsilon) #to avoid log(0)\n",
    "            L = - np.mean(Y_true * np.log(y_pred) + (1 - Y_true) * np.log(1 - y_pred))\n",
    "            self.lose.append(L)\n",
    "            dL_dy = (y_pred - Y_true) / Y_true.shape[0]\n",
    "            #(y_pred * (1- y_pred) * len(Y_true))\n",
    "        \n",
    "        return dL_dy\n",
    "\n",
    "\n",
    "    # backword --> calculate gradients\n",
    "    def backword(self, dL_dy):\n",
    "\n",
    "        # activations derivatieves\n",
    "        dy_dz = 0\n",
    "        if self.activation == \"Linear\":\n",
    "            dy_dz = 1\n",
    "        elif self.activation == \"Relu\":\n",
    "            dy_dz = (self.z > 0).astype(float)\n",
    "        elif self.activation == \"Tanh\":\n",
    "            dy_dz = 1 - self.y_pred**2\n",
    "        elif self.activation == \"Sigmoid\":\n",
    "            dy_dz = self.y_pred * (1 - self.y_pred)\n",
    "        \n",
    "        dL_dz = dL_dy * dy_dz\n",
    "        dL_dw = self.X.T @ dL_dz\n",
    "        dL_db = np.sum(dL_dz, axis=0, keepdims=1) * 1\n",
    "        dL_dx = dL_dz @ self.w.T\n",
    "\n",
    "        return dL_dw, dL_db, dL_dx\n",
    "\n",
    "    def step(self, dL_dw, dL_db):\n",
    "        #update parameters\n",
    "        self.w = self.w - self.Lr * dL_dw\n",
    "        self.b = self.b - self.Lr * dL_db\n",
    "\n",
    "        return self.w, self.b\n",
    " \n",
    "\n",
    "    def viz(self):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.title(\"ðŸ§  Neural Layer Architecture\", fontsize=14, pad=20)\n",
    "\n",
    "        # Draw neurons\n",
    "        for i in range(self.n_inputs):  # Input layer\n",
    "            plt.scatter(0, i, s=500, c='lightblue', edgecolors='black', zorder=5)\n",
    "            plt.text(0, i, f'x{i+1}', ha='center', va='center', fontweight='bold')\n",
    "            \n",
    "        for j in range(self.n_outputs):  # Output layer\n",
    "            plt.scatter(2, j, s=500, c='lightgreen', edgecolors='black', zorder=5)\n",
    "            plt.text(2, j, f'a{j+1}', ha='center', va='center', fontweight='bold')\n",
    "            \n",
    "            # Draw connections\n",
    "            for i in range(self.n_inputs):\n",
    "                plt.plot([0, 2], [i, j], 'gray', alpha=0.3)\n",
    "\n",
    "        plt.xlim(-0.5, 2.5)\n",
    "        plt.ylim(-0.5, 4.5)\n",
    "        plt.axis('off')\n",
    "        plt.text(0, -0.3, f\"Input Layer {self.n_inputs}, neurons\", ha='center', fontsize=12)\n",
    "        plt.text(2, -0.3, f\"Dense Layer {self.n_outputs}, neurons\", ha='center', fontsize=12)\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e7c779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model building stack of layers MLP\n",
    "class MLP:\n",
    "\n",
    "    def __init__(self, n_neuorns_per_layer, list_activations=None):\n",
    "    \n",
    "        #item in zero index is the intput it self\n",
    "        self.n_neuorns_per_layer = n_neuorns_per_layer\n",
    "        self.list_activations = list_activations or [\"Relu\"] * (len(n_neuorns_per_layer) - 2) + [\"Sigmoid\"]\n",
    "        #to stack layers in it\n",
    "        self.layers = [] \n",
    "        self.lose = []\n",
    "\n",
    "        #for the first layer is X it self and it is not counted in the loop as a layer, I see it as a layer but the program doesn't, it see the second layer\n",
    "        # we said - 1 to skip first layer as it is the input\n",
    "        for i in range(len(n_neuorns_per_layer) - 1): \n",
    "            layer = DesnsLayer(self.n_neuorns_per_layer[i], self.n_neuorns_per_layer[i + 1], self.list_activations[i])\n",
    "            #creating a list of objects to call it later to access the class functions\n",
    "            self.layers.append(layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        #to save the input to the memeory to know it later\n",
    "        self.X = X \n",
    "\n",
    "        #because for each layer its input is the output of previous layer\n",
    "        self.previous_output = X \n",
    "        for layer in self.layers:\n",
    "            self.previous_output = layer.forward(self.previous_output)\n",
    "        #return the last output as the inner ones are not important to me\n",
    "        return self.previous_output  \n",
    "    \n",
    "    def backward(self, dL_dy):\n",
    "        y_pred = self.previous_output\n",
    "\n",
    "        parameters_gradient =[]\n",
    "        current_grad = dL_dy\n",
    "        for layer in reversed(self.layers):\n",
    "            dL_dw, dL_db, dL_dx = layer.backword(current_grad)\n",
    "            parameters_gradient.append((dL_dw, dL_db))\n",
    "            current_grad = dL_dx\n",
    "\n",
    "        return list(reversed(parameters_gradient))\n",
    "    \n",
    "    def step(self, parameters_gradient, Lr=0.01):\n",
    "        for i, (dL_dw, dL_db) in enumerate(parameters_gradient):\n",
    "            self.layers[i].w -= Lr * dL_dw\n",
    "            self.layers[i].b -= Lr * dL_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a202fac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the classiifer\n",
    "class NeruronNetworkLearning:\n",
    "\n",
    "    def __init__(self, layer_size, activations = None, Lr = 0.01, batch_size = 32):\n",
    "        self.mlp = MLP(layer_size, activations)\n",
    "        self.layer_size = layer_size\n",
    "        self.activations = activations\n",
    "        self.batch_size = batch_size\n",
    "        self.Lr = Lr\n",
    "        self.loss_history = []\n",
    "        self.accuracy_history = []\n",
    "        self.eval_loss_history = []\n",
    "        self.eval_acc_history = []\n",
    "\n",
    "\n",
    "    def binary_cross_entropy(self, y_pred, y_true):\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon) #to avoid log(0)\n",
    "        L = - np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return L\n",
    "    \n",
    "    #its gradiant\n",
    "    def binary_cross_entropy_grad(self, y_pred, y_true):\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        L = (y_pred - y_true) / (y_pred * (1 - y_pred) * len(y_true))\n",
    "        return L\n",
    "    \n",
    "    def train(self, X_train, y_train, x_val, y_val, epochs = 1000, verbose = True):\n",
    "        n_samples = X_train.shape[0]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "             #to avoid learning one type of output, it will memoeries leading to overtting, also we didn't use suffle function because it will break the dataset sequance\n",
    "            index = np.random.permutation(n_samples)\n",
    "            x_shaffeled = X_train[index]\n",
    "            y_shaffeled = y_train[index]\n",
    "\n",
    "            #loop to create batchs\n",
    "            for start_idx in range(0, n_samples, self.batch_size):\n",
    "                end_index = min(start_idx + self.batch_size, n_samples)\n",
    "                #slizing the dataset based on the batch size\n",
    "                x_batch = x_shaffeled[start_idx : end_index]\n",
    "                y_batch = y_shaffeled[start_idx: end_index]\n",
    "\n",
    "                y_pred = self.mlp.forward(x_batch)\n",
    "\n",
    "                dL_dy = self.binary_cross_entropy_grad(y_pred, y_batch)\n",
    "                gradiants = self.mlp.backward(dL_dy)\n",
    "\n",
    "                self.mlp.step(gradiants, self.Lr)\n",
    "\n",
    "            y_pred_full_train = self.mlp.forward(X_train)\n",
    "            full_train_loss = self.binary_cross_entropy(y_pred_full_train, y_train)\n",
    "            full_train_acc = self.accuracy(y_pred_full_train, y_train)\n",
    "\n",
    "            self.loss_history.append(full_train_loss)\n",
    "            self.accuracy_history.append(full_train_acc)\n",
    "\n",
    "            #validating the model larning on new\n",
    "            if x_val is not None and y_val is not None:\n",
    "                y_pred_val = self.mlp.forward(x_val)\n",
    "                val_loss = self.binary_cross_entropy(y_pred_val, y_val)\n",
    "                val_acc = self.accuracy(y_pred_val, y_val)\n",
    "\n",
    "                self.eval_loss_history.append(val_loss)\n",
    "                self.eval_acc_history.append(val_acc)\n",
    "\n",
    "            if verbose and (epoch % 100 == 0 or epoch == epochs - 1):\n",
    "                val_info = f\" | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\" if x_val is not None else \"\"\n",
    "                print(f\"Epoch {epoch:4d} | Loss: {full_train_loss:.4f} | Acc: {full_train_acc:.2f}%{val_info}\")\n",
    "\n",
    "\n",
    "    \n",
    "    def accuracy(self, y_pred, y_true):\n",
    "        y_pred = (y_pred >= 0.5).astype(int)\n",
    "        return np.mean(y_pred == y_true) * 100\n",
    "    \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        y_pred = self.mlp.forward(X)\n",
    "        return (y_pred >= 0.5).astype(int)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict probabilities\"\"\"\n",
    "        return self.mlp.forward(X)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAIR_Courses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

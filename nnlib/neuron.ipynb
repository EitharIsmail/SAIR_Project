{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f52ffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f332d635",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuron:\n",
    "    \n",
    "    def __init__(self, n_inputs, activation, Lr = 0.01):\n",
    "        self.w = np.random.randn(n_inputs, 1)\n",
    "        self.b = np.zeros((1, 1))\n",
    "        self.lose =[] #for viz\n",
    "        self.activation = activation\n",
    "        self.Lr = Lr\n",
    "        self.n_inputs = n_inputs\n",
    "\n",
    "    #activation functions\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -250, 250)))\n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def tanh(self, z):\n",
    "        return np.tanh(z) \n",
    "    \n",
    "    #forward path to activate the neuron\n",
    "    def forward(self, X):\n",
    "        z = X @ self.w + self.b\n",
    "\n",
    "        #calling activation functions to activate the neuron to produce output(prediction)\n",
    "        if self.activation == \"Linear\":\n",
    "            y_pred = z\n",
    "        elif self.activation == \"Relu\":\n",
    "            y_pred = self.relu(z)\n",
    "        elif self.activation == \"Tanh\":\n",
    "            y_pred = self.tanh(z)\n",
    "        elif self.activation == \"Sigmoid\":\n",
    "            y_pred = self.sigmoid(z)\n",
    "\n",
    "        #local_deriative\n",
    "        self.dw = X\n",
    "        self.db = 1\n",
    "        self.dx = self.w.T\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    # backword --> calculate gradients\n",
    "    def backword(self, y_pred, Y_true):\n",
    "        #loss function first\n",
    "        if self.activation == \"Linear\":\n",
    "            L = np.mean((Y_true - y_pred) ** 2)\n",
    "        elif self.activation == \"Relu\":\n",
    "            L = - [(1 / self.n_inputs) * np.mean(Y_true * np.log(y_pred) + (1 - Y_true) * np.log(1 - y_pred))]\n",
    "        #elif self.activation == \"Tanh\":\n",
    "        #    self.L = \n",
    "        \n",
    "        #filling the loss list for viz\n",
    "        self.lose.append(L)\n",
    "        \n",
    "        # global deriviatives\n",
    "        if self.activation == \"Linear\":\n",
    "            L = np.mean((Y_true - y_pred) ** 2)\n",
    "            dy_dz = 1\n",
    "            dL_dy = 2 * (Y_true - y_pred) * dy_dz\n",
    "        elif self.activation == \"Relu\":\n",
    "            if y_pred == 0 :\n",
    "                dy_dz = 0 \n",
    "            else:\n",
    "                dy_dz = 1\n",
    "            dL_dy = (-(Y_true / y_pred + (1- Y_true)/(1-y_pred))) * dy_dz  # this is temporar till I find the correct answer\n",
    "        #elif self.activation == \"Tanh\":\n",
    "        #    self.dy_dz = \n",
    "        #    self.dL_dy = \n",
    "        \n",
    "        #update parameters\n",
    "        self.w = self.w - self.Lr * dy_dz\n",
    "        self.b = self.b - self.Lr * dy_dz\n",
    "\n",
    "        print(\"w = \", self.w)\n",
    "        print(\"b = \", self.b)\n",
    "        print(self.lose)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706e1132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w =  [[ 0.86247632]\n",
      " [-1.44910616]\n",
      " [-1.65350545]]\n",
      "b =  [[-0.01]]\n",
      "Loss value =  [np.float64(0.02299433027930645)]\n"
     ]
    }
   ],
   "source": [
    "X_sample = np.array([[0.5, -1.2, 0.8]])\n",
    "#Y = 1\n",
    "#n = neuron(3, activation='Linear')\n",
    "#y_pred = n.forward(X_sample)\n",
    "#back = n.backword(y_pred, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "32d29681",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DesnsLayer:\n",
    "\n",
    "    #n_output mean the number of neurons per layer\n",
    "    def __init__(self, n_inputs, n_outputs, activation, Lr = 0.01):\n",
    "        self.w = np.random.randn(n_inputs, n_outputs) * 0.1 # here we multiplied by 0.1 to avoid the explding of the gradients because if the values of w came large then z will be big\n",
    "        self.b = np.zeros((1, n_outputs))\n",
    "        self.activation = activation\n",
    "        self.Lr = Lr\n",
    "        self.lose =[] #for viz\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "\n",
    "    #activation functions\n",
    "    def sigmoid(self, X):\n",
    "        return 1 / (1 + np.exp(-np.clip(X, -250, 250)))\n",
    "    \n",
    "    def relu(self, X):\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def tanh(self, X):\n",
    "        return np.tanh(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        z = X @ self.w + self.b\n",
    "        #print(z)\n",
    "\n",
    "        #calling activation functions to activate the neuron to produce output(prediction)\n",
    "        if self.activation == \"Linear\":\n",
    "            y_pred = z\n",
    "        elif self.activation == \"Relu\":\n",
    "            y_pred = self.relu(z)\n",
    "        elif self.activation == \"Tanh\":\n",
    "            y_pred = self.tanh(z)\n",
    "        elif self.activation == \"Sigmoid\":\n",
    "            y_pred = self.sigmoid(z)\n",
    "\n",
    "        #local_deriative\n",
    "        self.dw = X\n",
    "        self.db = 1\n",
    "        self.dx = self.w.T\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    # backword --> calculate gradients\n",
    "    def backword(self, y_pred, Y_true):\n",
    "        #loss function first\n",
    "        if self.activation == \"Linear\":\n",
    "            L = np.mean((Y_true - y_pred) ** 2)\n",
    "        elif self.activation == \"Relu\":\n",
    "            L = - [(1 / self.n_inputs) * np.mean(Y_true * np.log(y_pred) + (1 - Y_true) * np.log(1 - y_pred))]\n",
    "        #elif self.activation == \"Tanh\":\n",
    "        #    self.L = \n",
    "        \n",
    "        self.lose.append(L)\n",
    "        \n",
    "        # global deriviatives\n",
    "        if self.activation == \"Linear\":\n",
    "            L = np.mean((Y_true - y_pred) ** 2)\n",
    "            dy_dz = 1\n",
    "            dL_dy = 2 * (Y_true - y_pred) * dy_dz\n",
    "        elif self.activation == \"Relu\":\n",
    "            if y_pred == 0 :\n",
    "                dy_dz = 0 \n",
    "            else:\n",
    "                dy_dz = 1\n",
    "            dL_dy = (-(Y_true / y_pred + (1- Y_true)/(1-y_pred))) * dy_dz  # this is temporar till I find the correct answer\n",
    "        #elif self.activation == \"Tanh\":\n",
    "        #    self.dy_dz = \n",
    "        #    self.dL_dy = \n",
    "        \n",
    "        #update parameters\n",
    "        self.w = self.w - self.Lr * dy_dz\n",
    "        self.b = self.b - self.Lr * dy_dz\n",
    "\n",
    "        print(\"w = \", self.w)\n",
    "        print(\"b = \", self.b)\n",
    "        print(\"Loss value = \", self.lose)\n",
    "        print(\"Note :if you want to see layer architechture call the function (viz)\")\n",
    "\n",
    "    def viz(self):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.title(\"ðŸ§  Neural Layer Architecture\", fontsize=14, pad=20)\n",
    "\n",
    "        # Draw neurons\n",
    "        for i in range(self.n_inputs):  # Input layer\n",
    "            plt.scatter(0, i, s=500, c='lightblue', edgecolors='black', zorder=5)\n",
    "            plt.text(0, i, f'x{i+1}', ha='center', va='center', fontweight='bold')\n",
    "            \n",
    "        for j in range(self.n_outputs):  # Output layer\n",
    "            plt.scatter(2, j, s=500, c='lightgreen', edgecolors='black', zorder=5)\n",
    "            plt.text(2, j, f'a{j+1}', ha='center', va='center', fontweight='bold')\n",
    "            \n",
    "            # Draw connections\n",
    "            for i in range(self.n_inputs):\n",
    "                plt.plot([0, 2], [i, j], 'gray', alpha=0.3)\n",
    "\n",
    "        plt.xlim(-0.5, 2.5)\n",
    "        plt.ylim(-0.5, 4.5)\n",
    "        plt.axis('off')\n",
    "        plt.text(0, -0.3, f\"Input Layer {self.n_inputs}, neurons\", ha='center', fontsize=12)\n",
    "        plt.text(2, -0.3, f\"Dense Layer {self.n_outputs}, neurons\", ha='center', fontsize=12)\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d4b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = np.array([[0.5, -1.2, 0.8]])\n",
    "#Y = 1\n",
    "n = DesnsLayer(3, 4, activation='Linear')\n",
    "#y_pred = n.forward(X_sample)\n",
    "print(y_pred)\n",
    "back = n.backword(y_pred, Y)\n",
    "v = n.viz()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7c779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model building stack of layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087110e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "#Breast Cancer Dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "def data_loading(Dataset):\n",
    "    #exploring the data\n",
    "    bcd = load_breast_cancer()\n",
    "    #print(bcd.keys())\n",
    "\n",
    "    X = pd.DataFrame(data = bcd.data)\n",
    "    #X = bcd.data\n",
    "    print(X.shape[1])\n",
    "    Y = pd.DataFrame(data = bcd.target)\n",
    "    #print(Y)\n",
    "    #print(X.info)\n",
    "    return X, Y\n",
    "\n",
    "def train_test_split(Dataset):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1948c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to call the neuron class\n",
    "X, Y_true = data_loading()\n",
    "print(X)\n",
    "n = neuron(X.shape[1], activation='Linear')\n",
    "y_pred = n.forward(X)\n",
    "back = n.backword(y_pred, Y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4868e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0           1           2           3\n",
      "0   -319.549072  321.133608 -263.750678 -161.207315\n",
      "1   -305.500220  286.879528 -268.443461 -180.950544\n",
      "2   -271.221174  256.564580 -242.135548 -164.561453\n",
      "3    -93.828847   90.149491  -82.322888  -53.501102\n",
      "4   -256.437776  233.122119 -239.812816 -169.101494\n",
      "..          ...         ...         ...         ...\n",
      "564 -324.080460  309.997023 -294.507778 -203.426967\n",
      "565 -273.163303  260.040351 -246.124725 -170.493688\n",
      "566 -179.264762  167.862240 -162.752944 -113.023283\n",
      "567 -287.289567  272.962961 -254.988001 -172.170589\n",
      "568  -43.632245   46.310725  -37.854894  -25.646197\n",
      "\n",
      "[569 rows x 4 columns]\n",
      "w =  [[-2.62108341e-01 -4.71825521e-02  1.45134004e-01 -1.85414145e-01]\n",
      " [ 5.52044993e-02 -5.37224363e-03  4.45424229e-02  2.80827767e-02]\n",
      " [-1.04544749e-01  6.04698679e-02 -1.35176557e-01 -3.06359526e-02]\n",
      " [-1.84753503e-02 -4.05249159e-02 -7.47747412e-02 -9.52241447e-02]\n",
      " [-1.75625044e-01  2.27079424e-01  1.59745493e-01  1.55367065e-01]\n",
      " [-3.15943203e-02 -2.38360887e-03 -1.09548589e-01  6.86610215e-02]\n",
      " [ 9.94082565e-02 -8.30761992e-02 -3.89107199e-02  1.40178817e-01]\n",
      " [ 9.26779996e-02 -7.63728465e-02  1.31373260e-01 -1.50367416e-01]\n",
      " [-5.48762457e-03 -8.86940718e-02  3.14219893e-01  1.69620509e-01]\n",
      " [ 5.07576416e-02  8.72793786e-02 -1.51864842e-01 -5.63578112e-02]\n",
      " [ 6.20910103e-02 -1.43207358e-01  1.36951400e-01  1.05985941e-01]\n",
      " [-5.16807848e-02 -9.27814153e-02  6.63601744e-02 -6.79535333e-02]\n",
      " [ 3.97870857e-02  7.18675848e-02  1.66629440e-02  2.95582091e-02]\n",
      " [-9.27314500e-02  1.75627764e-01 -1.27715935e-01 -9.81628991e-02]\n",
      " [ 5.52663592e-02 -1.32357668e-02 -2.70065898e-02 -1.28154382e-01]\n",
      " [-2.41592982e-02 -2.04857282e-05 -1.22989704e-02 -1.72016359e-01]\n",
      " [-4.27407540e-02  1.49107231e-02  7.54471006e-02  4.65040248e-02]\n",
      " [-1.54826762e-01  4.64569872e-02  5.25168054e-02 -1.88238266e-01]\n",
      " [-1.09829102e-01  4.47774538e-03 -7.65225604e-02 -1.58743589e-03]\n",
      " [ 1.02790823e-01  4.85088176e-02 -1.78462411e-02 -1.87382679e-01]\n",
      " [ 1.09390921e-01  2.58575238e-02  3.63164932e-02  2.42676378e-02]\n",
      " [ 1.43339229e-01  5.81783231e-02  1.30947256e-01 -1.02539075e-02]\n",
      " [-9.78771784e-02  1.98803420e-02 -8.05329284e-02 -1.86777697e-03]\n",
      " [-1.45134576e-01  1.42117645e-01 -8.91885312e-02 -3.96896325e-02]\n",
      " [-9.99672424e-03 -4.89811339e-05 -1.28055592e-01 -1.58386038e-01]\n",
      " [ 2.22732521e-02 -5.50403233e-02 -2.38802522e-01 -2.92907229e-02]\n",
      " [-4.49592319e-02 -6.61454768e-03  5.71774623e-03  2.94589844e-02]\n",
      " [-4.88940204e-02 -2.14917635e-03 -4.32075251e-02  1.16790993e-01]\n",
      " [-3.99423053e-02 -1.37817894e-02  1.68518559e-01 -1.03348921e-01]\n",
      " [ 1.43093643e-02 -5.46675128e-03 -2.89728762e-02  3.28936648e-02]]\n",
      "b =  [[-0.01 -0.01 -0.01 -0.01]]\n",
      "Loss value =  [np.float64(28054.33988841155)]\n",
      "Note :if you want to see layer architechture call the function (viz)\n",
      "              0          1          2           3\n",
      "0   -199.020938 -35.575969  78.496390  173.750932\n",
      "1   -207.150675  15.804089   8.888818  163.914299\n",
      "2   -179.888698  18.835747   3.871894  148.573139\n",
      "3    -67.232127   2.523057  -4.230787   53.138687\n",
      "4   -171.342390  41.618116 -27.783074  141.865680\n",
      "..          ...        ...        ...         ...\n",
      "564 -205.356941  29.115403   6.200734  180.836927\n",
      "565 -181.463531  24.338246  -3.115419  151.072239\n",
      "566 -124.965219  20.440404 -14.279698   99.405309\n",
      "567 -195.654050  16.147834   0.497365  157.708862\n",
      "568  -31.772189   0.716143  -2.003696   26.678863\n",
      "\n",
      "[569 rows x 4 columns]\n",
      "w =  [[-0.31209634 -0.12109857 -0.02509498  0.07992979]\n",
      " [ 0.0115194   0.06162397  0.24267771  0.00755095]\n",
      " [ 0.00668391  0.03291488 -0.0090312  -0.0063318 ]\n",
      " [-0.0299786   0.12152833 -0.1739186   0.00481703]\n",
      " [-0.07624148 -0.03204333 -0.00627524 -0.15726788]\n",
      " [-0.14992147  0.00466963  0.10958058  0.07460453]\n",
      " [-0.21590687  0.09302534  0.0211969  -0.03235964]\n",
      " [ 0.00145842 -0.15283629 -0.13071864 -0.03507143]\n",
      " [ 0.04773523  0.01309287  0.04826822 -0.02949882]\n",
      " [-0.04486026 -0.02693112 -0.03460609 -0.01645859]\n",
      " [-0.02959497 -0.09915829  0.16401935  0.06806526]\n",
      " [ 0.09914101 -0.07601124  0.00332967 -0.05480093]\n",
      " [ 0.04708906  0.12160286  0.03036148 -0.06418634]\n",
      " [ 0.12668955 -0.0294777   0.15588885  0.106214  ]\n",
      " [ 0.06477475 -0.16101299  0.12941757 -0.06140795]\n",
      " [ 0.110624   -0.01305518  0.06344828 -0.16377624]\n",
      " [-0.14720316 -0.01467909 -0.1133319  -0.0097311 ]\n",
      " [-0.21400441  0.12443317 -0.13467436 -0.04499506]\n",
      " [ 0.06111634 -0.08204825  0.03093687 -0.04910738]\n",
      " [ 0.10304559 -0.02524075  0.0334308  -0.03019911]\n",
      " [-0.01827079  0.20674727  0.06738042  0.11530581]\n",
      " [ 0.05938171 -0.05983765 -0.19924732 -0.04807767]\n",
      " [-0.1744395  -0.10451212 -0.16867807  0.06027203]\n",
      " [-0.09317218 -0.0873865   0.11122089  0.05119129]\n",
      " [-0.1328443  -0.08636403  0.00151903 -0.00562475]\n",
      " [ 0.12995422 -0.09359542 -0.01665854  0.03784986]\n",
      " [-0.09518101 -0.13939629 -0.09781187  0.15906226]\n",
      " [-0.03476957 -0.02191676  0.02786538  0.18734353]\n",
      " [-0.01580812 -0.13054257  0.00276221  0.06149759]\n",
      " [-0.13078138  0.17436755 -0.11004826  0.03335253]]\n",
      "b =  [[-0.01 -0.01 -0.01 -0.01]]\n",
      "Loss value =  [np.float64(12891.326528150637)]\n",
      "Note :if you want to see layer architechture call the function (viz)\n",
      "              0           1           2           3\n",
      "0    128.261406 -172.393249 -110.238932  -90.879042\n",
      "1    127.618394 -166.500667  -91.154902 -127.979119\n",
      "2    102.562813 -147.263424  -76.851066 -113.723723\n",
      "3     36.547689  -52.019130  -24.973607  -34.715823\n",
      "4     83.330019 -138.373071  -61.218319 -121.769917\n",
      "..          ...         ...         ...         ...\n",
      "564  106.489302 -174.650310  -88.813297 -137.158091\n",
      "565  100.942270 -148.752554  -76.448641 -118.635639\n",
      "566   68.750495  -98.303693  -47.079229  -80.882360\n",
      "567  112.960672 -156.805100  -83.005493 -118.769918\n",
      "568   16.823783  -24.884223  -11.897212  -15.278638\n",
      "\n",
      "[569 rows x 4 columns]\n",
      "w =  [[-0.10405696 -0.14221622  0.02519608  0.00680745]\n",
      " [ 0.05161795  0.07084763 -0.00315662  0.0033334 ]\n",
      " [ 0.12818613 -0.0500235   0.02346344 -0.09011298]\n",
      " [-0.06308456 -0.01385915  0.03554152 -0.10158737]\n",
      " [ 0.06489451  0.12676452 -0.02316415 -0.00295149]\n",
      " [ 0.00556217  0.04586529  0.09651279 -0.09290915]\n",
      " [ 0.01896053  0.01630153  0.05954943  0.05108067]\n",
      " [-0.14459458  0.11639207 -0.21235065 -0.10125426]\n",
      " [-0.01007788  0.10117186 -0.06925866 -0.18408755]\n",
      " [ 0.09387008 -0.12043033 -0.06136607  0.00356892]\n",
      " [-0.0745371  -0.02275501 -0.12102359  0.07735768]\n",
      " [-0.05438912  0.06917818 -0.10536009  0.05310589]\n",
      " [-0.02869408 -0.0159288   0.08435681 -0.07674018]\n",
      " [-0.26465521 -0.03204865 -0.00370305  0.05686908]\n",
      " [ 0.10003324  0.09439277 -0.08224494 -0.03085912]\n",
      " [-0.17347828  0.10342249  0.06482093  0.01337146]\n",
      " [ 0.00787174  0.06753846  0.08270948  0.03315514]\n",
      " [-0.11844185 -0.10690954  0.03334514 -0.20825384]\n",
      " [ 0.02380742 -0.06038108 -0.04104497 -0.04203439]\n",
      " [ 0.09833612 -0.02376009 -0.03107119 -0.03348152]\n",
      " [-0.23469681 -0.13600749  0.01716926  0.09054766]\n",
      " [-0.01100858 -0.01228917 -0.04919004 -0.01394538]\n",
      " [-0.06234446 -0.03960345 -0.00582525  0.07403405]\n",
      " [ 0.09914886 -0.08428619 -0.09072739 -0.01869097]\n",
      " [ 0.00625711 -0.03699385  0.07793836  0.15756002]\n",
      " [-0.00985    -0.08231797  0.06132246  0.04517849]\n",
      " [-0.19463762 -0.17986355 -0.10950271 -0.0803552 ]\n",
      " [ 0.04938076 -0.05996372 -0.00550689 -0.11512778]\n",
      " [-0.12902275  0.18374827  0.02370557 -0.03217291]\n",
      " [-0.06728011 -0.01218788 -0.14961425 -0.05941341]]\n",
      "b =  [[-0.01 -0.01 -0.01 -0.01]]\n",
      "Loss value =  [np.float64(4182.3805262163405)]\n",
      "Note :if you want to see layer architechture call the function (viz)\n"
     ]
    }
   ],
   "source": [
    "#trying dense layer class  DesnsLayer\n",
    "\n",
    "#X_sample = np.array([[0.5, -1.2, 0.8]])\n",
    "#Y = 1\n",
    "n = DesnsLayer(X.shape[1], 4, activation='Linear')\n",
    "y_pred = n.forward(X)\n",
    "print(y_pred)\n",
    "back = n.backword(y_pred, Y)\n",
    "v = n.viz()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d234baa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAIR_Courses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
